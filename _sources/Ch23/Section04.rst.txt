Section 04: Questions for Correcting Mistake
::::::::::::::::::::::::::::::::::::::::::::

.. youtube:: W_80R4lX4H8
        :height: 315
        :width: 560
        :align: left

So the question then becomes, how might an error agent learn from its mistakes? Learning by correcting mistakes, or learning from failures, really entails answering three separate questions. The first question is, how can the agent isolate the error in its former model? So the agent had some model of the world. It made a mistake based on its model, how can it identify the error in its model? Note that this particular problem is very closely connected to the equation of diagnosis that we discussed earlier. The second question is how can the agent explain to itself that default it has identified the error in fact led it to the problem to the failure. Having identified the fault and explained how the fault led to the failure. The third question is how can the agent repair the fault in order to prevent the error, the failure from recurring. You may have noticed that earlier, we had related learning by correcting mistakes with matter cognition. You can see how that relationship occurs. The agent has some knowledge of the world. That knowledge leads it to failure. The agent is using that failure to repair it's own knowledge. It is as if the agent is looking into itself, looking into it's own reasoning, into it's own knowledge and correcting itself. Note once again that the learning here is incremental. We are learning from one example at a time. However instead of simply learning from an example we are also using explanation-based learning. We're trying to explain why a particular fault led to a particular failure. An explanation connects them with explanation that's learning, not just with the notion of incremental learning. Let us see how these three questions occur in the example of the pail. The first question is how can the agent identify that the fact that this particular pail has a moveable handle, not a fixed handle, is why this is not a good example of a cup. The second question is how can the agent with an explanation that proves why having that moveable handle makes this the poor example of a cup. Why does that lead to a failure? The third question is how can the agent change its model of a cup so that it never again picks an object with a movable handle as an example of a cup. So when we talked about explanation-based learning, we used another example of this as well. We imagined a desktop assistant that I can just say hey, fetch me that important file from last Tuesday. And it can construct it's own understanding of what file I might be talking about. It has a notion of what files have been important in the past and certain criteria of those files. So constructs an understanding of what important is and tries to transfer that onto files from last Tuesday. Now imagine that I told this agent hey, fetch me that important file from last Tuesday. And it returns me a file that actually wasn't important. And I say hey, that file isn't actually important at all. The agent would first try to isolate what error it made in diagnosing that particular document as important. You might, for example, notice that every other document I ever labeled as important was very recent whereas this one was really old. So even though it met all the criteria for an important document there might be more criteria that it didn't consider yet, and one of those might be that only new documents are very important. It would then explain that the problem came from the assumption that an old document could be important, and it would It would then repair its model to say that in the future old documents can't be important, even if they meet the other criteria for importance. This problem identifying the error in one's knowledge that led to a failure was called credit assignment. Blame assignment might be a better term. A failure has occurred. What fault of gap in one's knowledge was responsible for the failure? That's blame assignment. In this lesson, we'll be focusing on gaps or errors in one's knowledge. In general, the error could be in one's reasoning or in one's architecture. Credit assignment applies to all of those different kind of errors. Several Herculists, Marvin Minsky for example, consider credit assignment to be the central problem in learning. This is because error agents live in dynamic worlds. Therefore, we'll never be able to create an error agent which is perfect. Even if you were to create an error agent which had complete knowledge and perfect reasoning that lives in some world, the world around it would changed over time. As it changes, the agent will start failing. Once it starts failing, it must have the ability of correcting itself, of correcting its known knowledge, correcting its own reasoning, correcting its own architecture. You can see again how this a record of meta cognition. The agent is not diagnosing some electrical circuit or a car or software program outside. Instead, it is self diagnosing, self repairing.

.. reveal:: revealcbrreading1
    :showtitle: Show Reading
    :hidetitle: Hide Reading

    .. raw:: html

        <center>
        <iframe height=600px width=800px src=../_static/readings/LearningByCorrectingMistakes/Winston_Ch18.pdf>
        </iframe>
        </center>
        
or download :download:`here <../_static/readings/LearningByCorrectingMistakes/Winston_Ch18.pdf>`